{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all datasets:\n",
    "1. Extract dataset: \n",
    "    - HTTP download into csv file format\n",
    "        - Resilience: Add retries after n-seconds if unsuccessful initially\n",
    "\n",
    "2. Transform dataset\n",
    "    - Drop all columns that are unnecessary\n",
    "    - (Translate column names) - this would be hardcoded and has potential license implications (just like transforming actually)\n",
    "    - Check for missing values:\n",
    "        - if over a certain threshold: handle with:\n",
    "            - mean/mode/deletion/regression/knn\n",
    "    - Change time datatypes into datetime format\n",
    "    - (Potentially) Aggregate datasets into monthly formats\n",
    "        - Not sure if that is best for this step or better saved for a later step\n",
    "\n",
    "3. Load dataset into /data/ folder\n",
    "\n",
    "- In order to make it modular (and because we are working with a large number of datasets):\n",
    "    - define functions for standard tasks\n",
    "        - download\n",
    "        - datetime transformation\n",
    "        - missing value handling\n",
    "        - (dropping columns)\n",
    "        - saving dataset\n",
    "- Throughout it all use logging (on console & also in a log file? use library?) \n",
    "- Focus on Error Handling\n",
    "- Don't forget to update github issue with this content\n",
    "- Add .sh\n",
    "- Perform operations not in place"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:41:21.385595Z",
     "start_time": "2024-11-11T16:41:21.381765Z"
    }
   },
   "source": [
    "# Installs\n",
    "#%pip install retry\n",
    "\n",
    "# TODO: Necessary? Does this fw the .sh -> create a proper venv for this project?"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:50:02.607690Z",
     "start_time": "2024-11-12T14:50:02.598108Z"
    }
   },
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from retry import retry\n",
    "import requests\t\n",
    "from enum import Enum, auto\n",
    "import io\n",
    "import copy\n",
    "import zipfile\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:23:26.833936Z",
     "start_time": "2024-11-12T13:23:26.829966Z"
    }
   },
   "source": [
    "# Configure the logging system\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:50:05.178570Z",
     "start_time": "2024-11-12T14:50:04.652896Z"
    }
   },
   "source": [
    "@retry(tries=3, delay=30, logger=logging.getLogger())\n",
    "def extract_dataset(dataset_url: str, timeout: (int,int) = (None, None), is_zip: bool = False):\n",
    "    \"\"\"\n",
    "    Download datasets via HTTP request.\n",
    "    Retry three times, after waiting for 30s each, if unsuccessful.\n",
    "    \n",
    "    Parameters:\n",
    "    dataset_url: (str): URL of a dataset in the csv-file format.\n",
    "    timeout: (int, int): The timeout for the HTTP request in seconds. First tuple value is connection timeout, second tuple value is read timeout. Default behaviour is, that no time-out is applied\n",
    "    is_zip: (bool): Flag indicating if the dataset is a zip file containing multiple CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    response: The decoded response content\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(f\"Attempting to fetch data from {dataset_url}\") \n",
    "    response = requests.get(dataset_url, timeout=timeout) \n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors \n",
    "    logging.info(f\"Successfully fetched data from {dataset_url}\") \n",
    "    \n",
    "    # Pick out the csv dataset file which is not metadata (as identified by file-name).\n",
    "    if is_zip:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:\n",
    "            csv_files = [f for f in zip_file.namelist() if f.endswith('.csv') and 'metadata' not in f.lower()]\n",
    "            if len(csv_files) == 1: # Ensure that there is only a singular csv dataset file\n",
    "                with zip_file.open(csv_files[0]) as csv_file:\n",
    "                    csv_data = csv_file.read().decode('utf-8')\n",
    "                    return csv_data\n",
    "            else:\n",
    "                raise ValueError(f\"Expected exactly one CSV file without 'metadata' in the name, found: {csv_files}\")\n",
    "    else:\n",
    "        return response.content.decode('utf-8')\n",
    "    return response"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:50:04,658 - INFO - Attempting to fetch data from https://api.worldbank.org/v2/en/indicator/SP.POP.TOTL?downloadformat=csv\n",
      "2024-11-12 15:50:04,662 - DEBUG - Starting new HTTPS connection (1): api.worldbank.org:443\n",
      "2024-11-12 15:50:05,141 - DEBUG - https://api.worldbank.org:443 \"GET /v2/en/indicator/SP.POP.TOTL?downloadformat=csv HTTP/11\" 200 87056\n",
      "2024-11-12 15:50:05,169 - INFO - Successfully fetched data from https://api.worldbank.org/v2/en/indicator/SP.POP.TOTL?downloadformat=csv\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:56:49.623303Z",
     "start_time": "2024-11-12T14:56:49.611375Z"
    }
   },
   "source": [
    "def extract_into_df(csv_data, separator=\",\", skiprows=0):\n",
    "    \"\"\"\n",
    "    Load a csv dataset into a pandas dataframe for further transformation.\n",
    "    \n",
    "    Parameters:\n",
    "    csv_data: Dataset in CSV format as provided by extract_dataset_function.\n",
    "    separator: The CSV value separator for this file\n",
    "    skiprows: The number of rows of metadata that are to be skipped at the beginning of the file\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The resulting DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    try: \n",
    "        logging.info(f\"Attempting to load data into DataFrame\") \n",
    "        #df = pd.read_csv(io.StringIO(csv_data.content.decode('utf-8')), sep=separator)\n",
    "        df = pd.read_csv(io.StringIO(csv_data), sep=separator, skiprows=skiprows)\n",
    "        logging.info(f\"Successfully loaded data into DataFrame with {len(df)} rows\") \n",
    "        return df \n",
    "    except requests.exceptions.RequestException as e: \n",
    "        logging.error(f\"Failed to load data: {e}\") \n",
    "        raise e \n",
    "    except Exception as e: \n",
    "        logging.error(f\"Unexpected error: {e}\") \n",
    "        raise e"
   ],
   "outputs": [],
   "execution_count": 154
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:00:21.493918Z",
     "start_time": "2024-11-12T14:00:21.486431Z"
    }
   },
   "source": [
    "def filter_drop_columns(df, white_list):\n",
    "    \"\"\"\n",
    "    Drop columns from a DataFrame except those in the whitelist.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to operate on.\n",
    "    white_list (list): List of columns to keep.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with only the columns in the whitelist.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Attempting to drop non-whitelisted columns\")\n",
    "\n",
    "        # Check if whitelist columns exist in DataFrame\n",
    "        missing_columns = [col for col in white_list if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"The following columns in the whitelist are missing from the DataFrame: {missing_columns}\")\n",
    "\n",
    "        # Drop columns not in the whitelist\n",
    "        columns_to_drop = [col for col in df.columns if col not in white_list]\n",
    "        df_dropped = df.drop(columns=columns_to_drop)\n",
    "\n",
    "        if len(list(df_dropped.columns)) < 15:\n",
    "            logging.info(f\"Successfully dropped columns. Remaining columns: {list(df_dropped.columns)}\")\n",
    "        else:\n",
    "            logging.info(f\"Successfully dropped columns. {len(list(df_dropped.columns))} columns remaining.\")\n",
    "\n",
    "        return df_dropped\n",
    "\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Please make sure all columns in the white list are contained in the DataFrame: {e}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        return df\n"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:23:28.435640Z",
     "start_time": "2024-11-12T13:23:28.428843Z"
    }
   },
   "source": [
    "class Strategy(Enum):\n",
    "    \"\"\"\n",
    "    Each enumeration represents a strategy for handling missing values of pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    BFILL = auto()\n",
    "    FFILL = auto()\n",
    "    DROP_ROW = auto()\n",
    "    LINEAR_INTERPOLATION = auto()\n",
    "    MODE = auto()\n",
    "    MEDIAN = auto()\n",
    "\n",
    "\n",
    "def filter_handle_missing_values(df, column, threshold=0, strategy=Strategy.DROP_ROW):\n",
    "    \"\"\"\n",
    "    Handle missing values in a specific column of a DataFrame based on a given strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to operate on.\n",
    "    column (str): The column to handle missing values for.\n",
    "    threshold (float): Threshold of missing values (0-1) after which the strategy is applied.\n",
    "    strategy (Strategy): Strategy for handling missing values.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with missing values handled in the specified column.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        temp_df = copy.deepcopy(df)  # Avoid making in place changes to the dataframe\n",
    "\n",
    "        # Calculate the percentage of missing values in the column\n",
    "        missing_ratio = temp_df[column].isnull().mean()\n",
    "\n",
    "        if missing_ratio > threshold:\n",
    "            logging.info(\n",
    "                f\"Column '{column}' has {missing_ratio * 100:.2f}% missing values, applying {strategy.name} strategy\")\n",
    "\n",
    "            if strategy == Strategy.BFILL:\n",
    "                temp_df[column] = temp_df[column].fillna(method='bfill')\n",
    "                logging.info(f\"Applied back fill strategy to column '{column}'\")\n",
    "\n",
    "            elif strategy == Strategy.FFILL:\n",
    "                temp_df[column] = temp_df[column].fillna(method='ffill')\n",
    "                logging.info(f\"Applied forward fill strategy to column '{column}'\")\n",
    "\n",
    "            elif strategy == Strategy.DROP_ROW:\n",
    "                temp_df = temp_df.dropna(subset=[column])\n",
    "                logging.info(f\"Dropped rows with missing values in column '{column}'\")\n",
    "\n",
    "            elif strategy == Strategy.LINEAR_INTERPOLATION:\n",
    "                temp_df[column] = temp_df[column].interpolate(method='linear')\n",
    "                logging.info(f\"Applied linear interpolation to column '{column}'\")\n",
    "\n",
    "            elif strategy == Strategy.MODE:\n",
    "                mode_value = temp_df[column].mode()[0]\n",
    "                temp_df[column] = temp_df[column].fillna(mode_value)\n",
    "                logging.info(f\"Applied mode imputation to column '{column}' with mode value {mode_value}\")\n",
    "\n",
    "            elif strategy == Strategy.MEDIAN:\n",
    "                median_value = temp_df[column].median()\n",
    "                temp_df[column] = temp_df[column].fillna(median_value)\n",
    "                logging.info(f\"Applied median imputation to column '{column}' with median value {median_value}\")\n",
    "\n",
    "            return temp_df\n",
    "        else:\n",
    "            logging.info(\n",
    "                f\"Column '{column}' has {missing_ratio * 100:.2f}% missing values, which is lower than the threshold of {threshold * 100:.2f}%. Not applying a strategy.\")\n",
    "            return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Unexpected error while handling missing values in column '{column}' with strategy '{strategy.name}': {e}\")\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:16:57.684614Z",
     "start_time": "2024-11-12T15:16:57.674006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_rows_by_values(df, column_name, column_values):\n",
    "    \"\"\"\n",
    "    Filter rows in a DataFrame based on column values and drop all rows where values do not match the given values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to filter.\n",
    "    column_name (str): The column name to check for the values.\n",
    "    column_values: The value or list of values to filter rows by.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with rows filtered by the given column values.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Ensure the column exists in the DataFrame\n",
    "        if column_name not in df.columns:\n",
    "            logging.error(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "            return df\n",
    "        \n",
    "        # If column_values is not a list, convert it to a list\n",
    "        if not isinstance(column_values, list):\n",
    "            column_values = [column_values]\n",
    "        \n",
    "        # Create a mask for the matching rows\n",
    "        mask = df[column_name].isin(column_values)\n",
    "        affected_rows = len(df) - mask.sum()  # Calculate the number of rows that do not match\n",
    "        \n",
    "        # Filter the DataFrame\n",
    "        filtered_df = df[mask]\n",
    "        \n",
    "        logging.info(f\"Column '{column_name}': Filtering rows where values are in {column_values}.\")\n",
    "        logging.info(f\"Number of rows dropped: {affected_rows}\")\n",
    "        return filtered_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while filtering rows by '{column_name}' with value '{column_values}': {e}\")\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": 201
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:09:46.637800Z",
     "start_time": "2024-11-12T14:09:46.629345Z"
    }
   },
   "source": [
    "# TODO: I will have to see how well this works with the different datasets. Check how many NaT (not a time) values there are, and handle that case (possibly using above function)\n",
    "\n",
    "# Function to check if a column name can be converted to a datetime object\n",
    "def try_convert_to_datetime(col_name):\n",
    "    try:\n",
    "        return pd.to_datetime(col_name, dayfirst=True, errors='raise').date()\n",
    "    except ValueError:\n",
    "        return col_name\n",
    "\n",
    "\n",
    "def filter_transform_to_datetime(df, column=None, do_columns=False):\n",
    "    \"\"\"\n",
    "    Transform a column in various formats to a uniform datetime datatype.\n",
    "    Alternatively transform column names to datetime datatype.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the date column.\n",
    "    column (str): The column name containing date values in various formats.\n",
    "    doColumns (bool): If true, all column names that represent a date will be transformed to uniform datetime objects\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with the date column transformed to datetime.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        temp_df = copy.deepcopy(df)  # Avoid making in place changes to the dataframe\n",
    "        if do_columns:\n",
    "\n",
    "            # Transform columns that can be parsed as datetime objects\n",
    "            new_columns = {col: try_convert_to_datetime(col) for col in temp_df.columns}\n",
    "            temp_df.rename(columns=new_columns, inplace=True)\n",
    "            logging.info(f\"Successfully transformed {len(new_columns)} column names to datetime\")\n",
    "        elif column is not None:\n",
    "            logging.info(f\"Transforming column '{column}' to datetime\")\n",
    "            temp_df[column] = pd.to_datetime(temp_df[column], errors='coerce').dt.date\n",
    "            logging.info(f\"Successfully transformed column '{column}' to datetime\")\n",
    "        else:\n",
    "            logging.error(\"Please provide a meaningful column name\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while transforming column '{column}' to datetime: {e}\")\n",
    "        return df\n",
    "\n",
    "    return temp_df"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:23:28.935569Z",
     "start_time": "2024-11-12T13:23:28.929770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_df_to_csv(df, file_name, file_path='../data/', overwrite=False):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to save.\n",
    "    file_path (str): The path where the CSV file will be saved. The default path is that to the local /data/ folder, as required by the project specifications.\n",
    "    file_name (str): The name of the file to be stored, excluding the file ending, which is hardcoded as '.csv'\n",
    "    overwrite (bool): Flag to allow overwriting of existing files.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    if not file_path:  # Check if file_path is an empty string \n",
    "        file_path = './'  # Default to current working directory \n",
    "\n",
    "    full_path = os.path.join(file_path, file_name + '.csv')\n",
    "\n",
    "    # Check if the file already exists \n",
    "    if os.path.exists(full_path):\n",
    "        if not overwrite:\n",
    "            logging.error(\n",
    "                f\"File '{full_path}' already exists. Set overwrite-flag to True in order to perform this action\")\n",
    "            return\n",
    "        else:\n",
    "            logging.warning(f\"File '{full_path}' is being overwritten as the overwrite-flag is set to True\")\n",
    "\n",
    "    try:\n",
    "        df.to_csv(full_path, index=False)\n",
    "        logging.info(f\"DataFrame successfully saved to {full_path}\")\n",
    "    except PermissionError as e:\n",
    "        logging.error(f\"Permission error while trying to save the DataFrame to {full_path}: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found error while trying to save the DataFrame to {full_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while saving the DataFrame to {full_path}: {e}\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Applying the ETL Pipeline to the datasets "
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chile Covid Mortality Dataset"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:51:32.730658Z",
     "start_time": "2024-11-12T14:51:25.204136Z"
    }
   },
   "source": [
    "chile_url = \"https://datos.gob.cl/dataset/8982a05a-91f7-422d-97bc-3eee08fde784/resource/8e5539b7-10b2-409b-ae5a-36dae4faf817/download/defunciones_covid19_2020_2024.csv\"\n",
    "\n",
    "# Extract the dataset into a data-frame\n",
    "chile_data = extract_dataset(chile_url, timeout=(200, 200))\n",
    "chile_df = extract_into_df(chile_data, separator=\";\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:51:25,207 - INFO - Attempting to fetch data from https://datos.gob.cl/dataset/8982a05a-91f7-422d-97bc-3eee08fde784/resource/8e5539b7-10b2-409b-ae5a-36dae4faf817/download/defunciones_covid19_2020_2024.csv\n",
      "2024-11-12 15:51:25,210 - DEBUG - Starting new HTTPS connection (1): datos.gob.cl:443\n",
      "2024-11-12 15:51:26,947 - DEBUG - https://datos.gob.cl:443 \"GET /dataset/8982a05a-91f7-422d-97bc-3eee08fde784/resource/8e5539b7-10b2-409b-ae5a-36dae4faf817/download/defunciones_covid19_2020_2024.csv HTTP/11\" 200 16071695\n",
      "2024-11-12 15:51:32,433 - INFO - Successfully fetched data from https://datos.gob.cl/dataset/8982a05a-91f7-422d-97bc-3eee08fde784/resource/8e5539b7-10b2-409b-ae5a-36dae4faf817/download/defunciones_covid19_2020_2024.csv\n",
      "2024-11-12 15:51:32,448 - INFO - Attempting to load data into DataFrame\n",
      "2024-11-12 15:51:32,724 - INFO - Successfully loaded data into DataFrame with 58289 rows\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:51:35.257987Z",
     "start_time": "2024-11-12T14:51:35.193268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform transformations\n",
    "\n",
    "# Required fields for analysis are the death-date and the diagnosis (COVID-19)\n",
    "chile_df = filter_drop_columns(chile_df, [\"FECHA_DEF\", \"DIAG1\"])\n",
    "\n",
    "# Transform the date-fields into datetime objects\n",
    "chile_df = filter_transform_to_datetime(chile_df, \"FECHA_DEF\")\n",
    "\n",
    "# No missing values are imputed, as there are not enough missing values in the dataset\n",
    "for column in chile_df.columns:\n",
    "    chile_df = filter_handle_missing_values(chile_df, column=column, strategy=Strategy.DROP_ROW)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:51:35,198 - INFO - Attempting to drop non-whitelisted columns\n",
      "2024-11-12 15:51:35,204 - INFO - Successfully dropped columns. Remaining columns: ['FECHA_DEF', 'DIAG1']\n",
      "2024-11-12 15:51:35,209 - INFO - Transforming column 'FECHA_DEF' to datetime\n",
      "2024-11-12 15:51:35,240 - INFO - Successfully transformed column 'FECHA_DEF' to datetime\n",
      "2024-11-12 15:51:35,248 - INFO - Column 'FECHA_DEF' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 15:51:35,254 - INFO - Column 'DIAG1' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:51:38.327721Z",
     "start_time": "2024-11-12T14:51:38.320590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the transformed dataframe back into a CSV-database file.\n",
    "load_df_to_csv(chile_df, file_name='chile_covid_mortality', overwrite=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:51:38,324 - ERROR - File '../data/chile_covid_mortality.csv' already exists. Set overwrite-flag to True in order to perform this action\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### USA Covid Mortality Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:51:49.624808Z",
     "start_time": "2024-11-12T14:51:45.350374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "usa_url = \"https://data.cdc.gov/api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD\"\n",
    "\n",
    "# Extract the dataset into a data-frame\n",
    "data = extract_dataset(usa_url, timeout=(200, 200))\n",
    "usa_df = extract_into_df(data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:51:45,354 - INFO - Attempting to fetch data from https://data.cdc.gov/api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD\n",
      "2024-11-12 15:51:45,357 - DEBUG - Starting new HTTPS connection (1): data.cdc.gov:443\n",
      "2024-11-12 15:51:46,691 - DEBUG - https://data.cdc.gov:443 \"GET /api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD HTTP/11\" 200 None\n",
      "2024-11-12 15:51:49,437 - INFO - Successfully fetched data from https://data.cdc.gov/api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD\n",
      "2024-11-12 15:51:49,442 - INFO - Attempting to load data into DataFrame\n",
      "2024-11-12 15:51:49,617 - INFO - Successfully loaded data into DataFrame with 79002 rows\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:51:52.868698Z",
     "start_time": "2024-11-12T14:51:52.789071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform transformations\n",
    "\n",
    "# This dataset has duplicate values, therefore drop all rows for the different regions in the US and keep only the total US rows.\n",
    "usa_df = filter_rows_by_values(usa_df, \"jurisdiction_residence\", \"United States\")\n",
    "\n",
    "# Keep only required fields for analysis\n",
    "usa_df = filter_drop_columns(usa_df, [\"data_period_start\", \"data_period_end\", \"group\", \"subgroup1\", \"covid_deaths\", \"crude_rate\"]) \n",
    "\n",
    "# Transform the date-fields into datetime objects. This also works for the american M/D/Y date format.\n",
    "usa_df = filter_transform_to_datetime(usa_df, \"data_period_start\") \n",
    "usa_df = filter_transform_to_datetime(usa_df, \"data_period_end\")\n",
    "\n",
    "# Drop the rows for which there is no data about covid mortality\n",
    "print(f\"Before: \\n{usa_df.isnull().sum()} \\n\")\n",
    "for column in usa_df.columns:\n",
    "    usa_df = filter_handle_missing_values(usa_df, column=column, strategy=Strategy.DROP_ROW)\n",
    "print(f\"After: \\n{usa_df.isnull().sum()} \\n\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:51:52,809 - INFO - Column 'jurisdiction_residence': Filtering rows where value is 'United States'.\n",
      "2024-11-12 15:51:52,810 - INFO - Number of rows dropped: 71820\n",
      "2024-11-12 15:51:52,812 - INFO - Attempting to drop non-whitelisted columns\n",
      "2024-11-12 15:51:52,815 - INFO - Successfully dropped columns. Remaining columns: ['data_period_start', 'data_period_end', 'group', 'subgroup1', 'covid_deaths', 'crude_rate']\n",
      "2024-11-12 15:51:52,816 - INFO - Transforming column 'data_period_start' to datetime\n",
      "2024-11-12 15:51:52,824 - INFO - Successfully transformed column 'data_period_start' to datetime\n",
      "2024-11-12 15:51:52,826 - INFO - Transforming column 'data_period_end' to datetime\n",
      "2024-11-12 15:51:52,832 - INFO - Successfully transformed column 'data_period_end' to datetime\n",
      "2024-11-12 15:51:52,842 - INFO - Column 'data_period_start' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 15:51:52,845 - INFO - Column 'data_period_end' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 15:51:52,848 - INFO - Column 'group' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 15:51:52,850 - INFO - Column 'subgroup1' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 15:51:52,854 - INFO - Column 'covid_deaths' has 27.32% missing values, applying DROP_ROW strategy\n",
      "2024-11-12 15:51:52,861 - INFO - Dropped rows with missing values in column 'covid_deaths'\n",
      "2024-11-12 15:51:52,863 - INFO - Column 'crude_rate' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "data_period_start       0\n",
      "data_period_end         0\n",
      "group                   0\n",
      "subgroup1               0\n",
      "covid_deaths         1962\n",
      "crude_rate           1962\n",
      "dtype: int64 \n",
      "\n",
      "After: \n",
      "data_period_start    0\n",
      "data_period_end      0\n",
      "group                0\n",
      "subgroup1            0\n",
      "covid_deaths         0\n",
      "crude_rate           0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:52:00.278878Z",
     "start_time": "2024-11-12T14:52:00.270742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the transformed dataframe back into a CSV-database file.\n",
    "load_df_to_csv(usa_df, file_name='usa_covid_mortality', overwrite=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:52:00,274 - ERROR - File '../data/usa_covid_mortality.csv' already exists. Set overwrite-flag to True in order to perform this action\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Colombia Covid Mortality Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:53:15.175909Z",
     "start_time": "2024-11-12T14:53:09.664481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "colombia_url = \"https://www.datos.gov.co/api/views/jp5m-e7yr/rows.csv?fourfour=jp5m-e7yr&cacheBust=1705599009&date=20241106&accessType=DOWNLOAD\"\n",
    "\n",
    "# Extract the dataset into a data-frame\n",
    "data = extract_dataset(colombia_url, timeout=(200, 200))\n",
    "colombia_df = extract_into_df(data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:53:09,667 - INFO - Attempting to fetch data from https://www.datos.gov.co/api/views/jp5m-e7yr/rows.csv?fourfour=jp5m-e7yr&cacheBust=1705599009&date=20241106&accessType=DOWNLOAD\n",
      "2024-11-12 15:53:09,671 - DEBUG - Starting new HTTPS connection (1): www.datos.gov.co:443\n",
      "2024-11-12 15:53:10,741 - DEBUG - https://www.datos.gov.co:443 \"GET /api/views/jp5m-e7yr/rows.csv?fourfour=jp5m-e7yr&cacheBust=1705599009&date=20241106&accessType=DOWNLOAD HTTP/11\" 200 None\n",
      "2024-11-12 15:53:14,641 - INFO - Successfully fetched data from https://www.datos.gov.co/api/views/jp5m-e7yr/rows.csv?fourfour=jp5m-e7yr&cacheBust=1705599009&date=20241106&accessType=DOWNLOAD\n",
      "2024-11-12 15:53:14,674 - INFO - Attempting to load data into DataFrame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fecha reporte web,ID de caso,Fecha de notificación,Código DIVIPOLA departamento,Nombre departamento,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:53:15,163 - INFO - Successfully loaded data into DataFrame with 143125 rows\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:52:11.145795Z",
     "start_time": "2024-11-12T14:52:11.038275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform transformations\n",
    "\n",
    "# Keep only required fields for analysis\n",
    "colombia_df = filter_drop_columns(colombia_df, [\"Fecha de muerte\", \"Recuperado\"])\n",
    "\n",
    "# Transform the date-fields into datetime objects.\n",
    "colombia_df = filter_transform_to_datetime(colombia_df, \"Fecha de muerte\")\n",
    "\n",
    "# No missing values are imputed, as there are not enough missing values in the dataset\n",
    "for column in colombia_df.columns:\n",
    "    colombia_df = filter_handle_missing_values(colombia_df, column=column, strategy=Strategy.MEDIAN)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:52:11,041 - INFO - Attempting to drop non-whitelisted columns\n",
      "2024-11-12 15:52:11,052 - INFO - Successfully dropped columns. Remaining columns: ['Recuperado', 'Fecha de muerte']\n",
      "2024-11-12 15:52:11,061 - INFO - Transforming column 'Fecha de muerte' to datetime\n",
      "2024-11-12 15:52:11,109 - INFO - Successfully transformed column 'Fecha de muerte' to datetime\n",
      "2024-11-12 15:52:11,124 - INFO - Column 'Recuperado' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 15:52:11,140 - INFO - Column 'Fecha de muerte' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T14:52:13.614470Z",
     "start_time": "2024-11-12T14:52:13.606819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the transformed dataframe back into a CSV-database file.\n",
    "load_df_to_csv(colombia_df, file_name='colombia_covid_mortality', overwrite=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:52:13,610 - ERROR - File '../data/colombia_covid_mortality.csv' already exists. Set overwrite-flag to True in order to perform this action\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mexico Covid Mortality Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:19:08.769492Z",
     "start_time": "2024-11-12T15:19:06.869861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mexico_url = \"https://datos.covid-19.conacyt.mx/Downloads/Files/Casos_Diarios_Estado_Nacional_Defunciones_20230625.csv\"\n",
    "\n",
    "# Extract the dataset into a data-frame\n",
    "data = extract_dataset(mexico_url, timeout=(200, 200))\n",
    "mexico_df = extract_into_df(data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:19:06,871 - INFO - Attempting to fetch data from https://datos.covid-19.conacyt.mx/Downloads/Files/Casos_Diarios_Estado_Nacional_Defunciones_20230625.csv\n",
      "2024-11-12 16:19:06,875 - DEBUG - Starting new HTTPS connection (1): datos.covid-19.conacyt.mx:443\n",
      "2024-11-12 16:19:08,212 - DEBUG - https://datos.covid-19.conacyt.mx:443 \"GET /Downloads/Files/Casos_Diarios_Estado_Nacional_Defunciones_20230625.csv HTTP/11\" 200 104290\n",
      "2024-11-12 16:19:08,738 - INFO - Successfully fetched data from https://datos.covid-19.conacyt.mx/Downloads/Files/Casos_Diarios_Estado_Nacional_Defunciones_20230625.csv\n",
      "2024-11-12 16:19:08,740 - INFO - Attempting to load data into DataFrame\n",
      "2024-11-12 16:19:08,766 - INFO - Successfully loaded data into DataFrame with 33 rows\n"
     ]
    }
   ],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:19:11.063128Z",
     "start_time": "2024-11-12T15:19:10.408815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform transformations\n",
    "\n",
    "# Transform the date column names into datetime format\n",
    "mexico_df = filter_transform_to_datetime(mexico_df, do_columns=True) \n",
    "\n",
    "# Keep the nombre column and all date columns as they will all be required for the analysis.\n",
    "# For this dataset, having a whitelist is a bit unfortunate, however we work around this issue by generating all column names automatically.\n",
    "start_date, end_date = '17-03-2020', '23-06-2023' # Define the date range \n",
    "date_range = pd.date_range(start=start_date, end=end_date).date.tolist() # Generate the date range \n",
    "date_range.append(\"nombre\")\n",
    "\n",
    "mexico_df = filter_drop_columns(mexico_df, white_list=date_range)\n",
    "\n",
    "# Leave only the row containing national mortality in order to avoid having duplicate values.\n",
    "mexico_df = filter_rows_by_values(mexico_df, \"nombre\", \"Nacional\")\n",
    "\n",
    "# No missing values need to be imputed, as there are not enough missing values in the dataset\n",
    "assert mexico_df.isnull().sum().sum() == 0"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:19:11,044 - INFO - Successfully transformed 1197' column names to datetime\n",
      "2024-11-12 16:19:11,049 - INFO - Attempting to drop non-whitelisted columns\n",
      "2024-11-12 16:19:11,056 - INFO - Successfully dropped columns. 1195 columns remaining.\n",
      "2024-11-12 16:19:11,059 - INFO - Column 'nombre': Filtering rows where values are in ['Nacional'].\n",
      "2024-11-12 16:19:11,060 - INFO - Number of rows dropped: 32\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:19:17.209421Z",
     "start_time": "2024-11-12T15:19:17.201658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the transformed dataframe back into a CSV-database file.\n",
    "load_df_to_csv(mexico_df, file_name='mexico_covid_mortality', overwrite=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:19:17,205 - ERROR - File '../data/mexico_covid_mortality.csv' already exists. Set overwrite-flag to True in order to perform this action\n"
     ]
    }
   ],
   "execution_count": 208
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### World Population Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:06:05.303008Z",
     "start_time": "2024-11-12T15:06:04.563661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "world_pop_url = \"https://api.worldbank.org/v2/en/indicator/SP.POP.TOTL?downloadformat=csv\"\n",
    "\n",
    "# Extract the dataset into a data-frame\n",
    "data = extract_dataset(world_pop_url, timeout=(200, 200), is_zip=True)\n",
    "world_pop_df = extract_into_df(data, skiprows=3)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:06:04,566 - INFO - Attempting to fetch data from https://api.worldbank.org/v2/en/indicator/SP.POP.TOTL?downloadformat=csv\n",
      "2024-11-12 16:06:04,569 - DEBUG - Starting new HTTPS connection (1): api.worldbank.org:443\n",
      "2024-11-12 16:06:05,292 - DEBUG - https://api.worldbank.org:443 \"GET /v2/en/indicator/SP.POP.TOTL?downloadformat=csv HTTP/11\" 200 87056\n",
      "2024-11-12 16:06:05,295 - INFO - Successfully fetched data from https://api.worldbank.org/v2/en/indicator/SP.POP.TOTL?downloadformat=csv\n",
      "2024-11-12 16:06:05,297 - INFO - Attempting to load data into DataFrame\n",
      "2024-11-12 16:06:05,300 - INFO - Successfully loaded data into DataFrame with 266 rows\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:11:57.024554Z",
     "start_time": "2024-11-12T15:11:57.016866Z"
    }
   },
   "cell_type": "code",
   "source": "world_pop_df.isnull().sum()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country Name    0\n",
       "2020            1\n",
       "2021            1\n",
       "2022            1\n",
       "2023            1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:12:06.238686Z",
     "start_time": "2024-11-12T15:12:06.224941Z"
    }
   },
   "cell_type": "code",
   "source": "world_pop_df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    Country Name         2020         2021         2022  \\\n",
       "0                          Aruba     106585.0     106537.0     106445.0   \n",
       "1    Africa Eastern and Southern  685112979.0  702977106.0  720859132.0   \n",
       "2                    Afghanistan   38972230.0   40099462.0   41128771.0   \n",
       "3     Africa Western and Central  466189102.0  478185907.0  490330870.0   \n",
       "4                         Angola   33428486.0   34503774.0   35588987.0   \n",
       "..                           ...          ...          ...          ...   \n",
       "261                       Kosovo    1790133.0    1786038.0    1768086.0   \n",
       "262                  Yemen, Rep.   32284046.0   32981641.0   33696614.0   \n",
       "263                 South Africa   58801927.0   59392255.0   59893885.0   \n",
       "264                       Zambia   18927715.0   19473125.0   20017675.0   \n",
       "265                     Zimbabwe   15669666.0   15993524.0   16320537.0   \n",
       "\n",
       "            2023  \n",
       "0       106277.0  \n",
       "1    739108306.0  \n",
       "2     42239854.0  \n",
       "3    502789511.0  \n",
       "4     36684202.0  \n",
       "..           ...  \n",
       "261    1756374.0  \n",
       "262   34449825.0  \n",
       "263   60414495.0  \n",
       "264   20569737.0  \n",
       "265   16665409.0  \n",
       "\n",
       "[266 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>106585.0</td>\n",
       "      <td>106537.0</td>\n",
       "      <td>106445.0</td>\n",
       "      <td>106277.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>685112979.0</td>\n",
       "      <td>702977106.0</td>\n",
       "      <td>720859132.0</td>\n",
       "      <td>739108306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38972230.0</td>\n",
       "      <td>40099462.0</td>\n",
       "      <td>41128771.0</td>\n",
       "      <td>42239854.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa Western and Central</td>\n",
       "      <td>466189102.0</td>\n",
       "      <td>478185907.0</td>\n",
       "      <td>490330870.0</td>\n",
       "      <td>502789511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>33428486.0</td>\n",
       "      <td>34503774.0</td>\n",
       "      <td>35588987.0</td>\n",
       "      <td>36684202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Kosovo</td>\n",
       "      <td>1790133.0</td>\n",
       "      <td>1786038.0</td>\n",
       "      <td>1768086.0</td>\n",
       "      <td>1756374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Yemen, Rep.</td>\n",
       "      <td>32284046.0</td>\n",
       "      <td>32981641.0</td>\n",
       "      <td>33696614.0</td>\n",
       "      <td>34449825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>58801927.0</td>\n",
       "      <td>59392255.0</td>\n",
       "      <td>59893885.0</td>\n",
       "      <td>60414495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>18927715.0</td>\n",
       "      <td>19473125.0</td>\n",
       "      <td>20017675.0</td>\n",
       "      <td>20569737.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>15669666.0</td>\n",
       "      <td>15993524.0</td>\n",
       "      <td>16320537.0</td>\n",
       "      <td>16665409.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:18:18.691983Z",
     "start_time": "2024-11-12T15:18:18.673306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform transformations\n",
    "\n",
    "# Keep the data for the years 2020-2023 and the country name as an identifier \n",
    "white_list = [str(x) for x in range(2020, 2024)]\n",
    "white_list.append(\"Country Name\")\n",
    "world_pop_df = filter_drop_columns(world_pop_df, white_list) \n",
    "\n",
    "# Select rows for the countries under analysis\n",
    "countries = [\"Chile\", \"United States\", \"Colombia\", \"Mexico\"]\n",
    "world_pop_df = filter_rows_by_values(world_pop_df, \"Country Name\", countries)\n",
    "    \n",
    "\n",
    "# No missing values are imputed, as there are not enough missing values in the dataset\n",
    "assert world_pop_df.isnull().sum().sum() == 0"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:18:18,676 - INFO - Attempting to drop non-whitelisted columns\n",
      "2024-11-12 16:18:18,679 - INFO - Successfully dropped columns. Remaining columns: ['Country Name', '2020', '2021', '2022', '2023']\n",
      "2024-11-12 16:18:18,687 - INFO - Column 'Country Name': Filtering rows where values are in ['Chile', 'United States', 'Colombia', 'Mexico'].\n",
      "2024-11-12 16:18:18,688 - INFO - Number of rows dropped: 262\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:18:49.360390Z",
     "start_time": "2024-11-12T15:18:48.440651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the transformed dataframe back into a CSV-database file.\n",
    "load_df_to_csv(colombia_df, file_name='world_population_total', overwrite=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:18:49,357 - INFO - DataFrame successfully saved to ../data/world_population_total.csv\n"
     ]
    }
   ],
   "execution_count": 205
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
