{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all datasets:\n",
    "1. Extract dataset: \n",
    "    - HTTP download into csv file format\n",
    "        - Resilience: Add retries after n-seconds if unsuccessful initially\n",
    "\n",
    "2. Transform dataset\n",
    "    - Drop all columns that are unnecessary\n",
    "    - (Translate column names) - this would be hardcoded and has potential license implications (just like transforming actually)\n",
    "    - Check for missing values:\n",
    "        - if over a certain threshold: handle with:\n",
    "            - mean/mode/deletion/regression/knn\n",
    "    - Change time datatypes into datetime format\n",
    "    - (Potentially) Aggregate datasets into monthly formats\n",
    "        - Not sure if that is best for this step or better saved for a later step\n",
    "\n",
    "3. Load dataset into /data/ folder\n",
    "\n",
    "- In order to make it modular (and because we are working with a large number of datasets):\n",
    "    - define functions for standard tasks\n",
    "        - download\n",
    "        - datetime transformation\n",
    "        - missing value handling\n",
    "        - (dropping columns)\n",
    "        - saving dataset\n",
    "- Throughout it all use logging (on console & also in a log file? use library?) \n",
    "- Focus on Error Handling\n",
    "- Don't forget to update github issue with this content\n",
    "- Add .sh\n",
    "- Perform operations not in place"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:41:21.385595Z",
     "start_time": "2024-11-11T16:41:21.381765Z"
    }
   },
   "source": [
    "# Installs\n",
    "#%pip install retry\n",
    "\n",
    "# TODO: Necessary? Does this fw the .sh -> create a proper venv for this project?"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T08:54:18.138041Z",
     "start_time": "2024-11-12T08:54:17.572031Z"
    }
   },
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from retry import retry\n",
    "import requests\t\n",
    "from enum import Enum, auto\n",
    "import io\n",
    "import copy\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T08:54:18.155415Z",
     "start_time": "2024-11-12T08:54:18.152553Z"
    }
   },
   "source": [
    "# Configure the logging system\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:13:31.416635Z",
     "start_time": "2024-11-12T09:13:31.411283Z"
    }
   },
   "source": [
    "@retry(tries=3, delay=30, logger=logging.getLogger())\n",
    "def extract_dataset(dataset_url: str, timeout: (int,int) = (None, None)):\n",
    "    \"\"\"\n",
    "    Download datasets via HTTP request.\n",
    "    Retry three times, after waiting for 30s each, if unsuccessful.\n",
    "    \n",
    "    Parameters:\n",
    "    dataset_url: (str): URL of a dataset in the csv-file format.\n",
    "    timeout: (int, int): The timeout for the HTTP request in seconds. First tuple value is connection timeout, second tuple value is read timeout. Default behaviour is, that no time-out is applied\n",
    "    \n",
    "    Returns:\n",
    "    response: The response of the HTTP GET request.\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f\"Attempting to fetch data from {dataset_url}\") \n",
    "    response = requests.get(dataset_url, timeout=timeout) \n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors \n",
    "    logging.info(f\"Successfully fetched data from {dataset_url}\") \n",
    "    return response"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:10:02.630172Z",
     "start_time": "2024-11-12T09:10:02.626594Z"
    }
   },
   "source": [
    "def extract_into_df(csv_data, separator=\",\"):\n",
    "    '''\n",
    "    Load a csv dataset into a pandas dataframe for further transformation.\n",
    "    \n",
    "    Parameters:\n",
    "    csv_data: Dataset in CSV format.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The resulting DataFrame\n",
    "    '''\n",
    "    \n",
    "    try: \n",
    "        logging.info(f\"Attempting to load data into DataFrame\") \n",
    "        df = pd.read_csv(io.StringIO(csv_data.content.decode('utf-8')), sep=separator)\n",
    "        logging.info(f\"Successfully loaded data into DataFrame with {len(df)} rows\") \n",
    "        return df \n",
    "    except requests.exceptions.RequestException as e: \n",
    "        logging.error(f\"Failed to load data: {e}\") \n",
    "        raise e \n",
    "    except Exception as e: \n",
    "        logging.error(f\"Unexpected error: {e}\") \n",
    "        raise e"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:10:02.855664Z",
     "start_time": "2024-11-12T09:10:02.850658Z"
    }
   },
   "source": [
    "def filter_drop_columns(df, white_list):\n",
    "    \"\"\"\n",
    "    Drop columns from a DataFrame except those in the whitelist.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to operate on.\n",
    "    white_list (list): List of columns to keep.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with only the columns in the whitelist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Attempting to drop non-whitelisted columns\")\n",
    "        \n",
    "        # Check if whitelist columns exist in DataFrame\n",
    "        missing_columns = [col for col in white_list if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"The following columns in the whitelist are missing from the DataFrame: {missing_columns}\")\n",
    "        \n",
    "        # Drop columns not in the whitelist\n",
    "        columns_to_drop = [col for col in df.columns if col not in white_list]\n",
    "        df_dropped = df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        logging.info(f\"Successfully dropped columns. Remaining columns: {list(df_dropped.columns)}\")\n",
    "        return df_dropped\n",
    "    \n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Please make sure all columns in the white list are contained in the DataFrame: {e}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        return df\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:17:25.044647Z",
     "start_time": "2024-11-12T09:17:25.033250Z"
    }
   },
   "source": [
    "class Strategy(Enum):\n",
    "    '''\n",
    "    Each enumeration represents a stratgie for handling missing values of pd.DataFrame\n",
    "    '''\n",
    "    \n",
    "    BFILL = auto()\n",
    "    FFILL = auto()\n",
    "    DROP_ROW = auto()\n",
    "    LINEAR_INTERPOLATION = auto()\n",
    "    MODE = auto()\n",
    "    MEDIAN = auto()\n",
    "\n",
    "def filter_handle_missing_values(df, column, threshold=0, strategy=Strategy.DROP_ROW):\n",
    "    \"\"\"\n",
    "    Handle missing values in a specific column of a DataFrame based on a given strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to operate on.\n",
    "    column (str): The column to handle missing values for.\n",
    "    threshold (float): Threshold of missing values (0-1) after which the strategy is applied.\n",
    "    strategy (Strategy): Strategy for handling missing values.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with missing values handled in the specified column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temp_df = copy.deepcopy(df) # Avoid making in place changes to the dataframe\n",
    "        \n",
    "        # Calculate the percentage of missing values in the column\n",
    "        missing_ratio = temp_df[column].isnull().mean()\n",
    "        \n",
    "        if missing_ratio > threshold:\n",
    "            logging.info(f\"Column '{column}' has {missing_ratio * 100:.2f}% missing values, applying {strategy.name} strategy\")\n",
    "            \n",
    "            if strategy == Strategy.BFILL:\n",
    "                temp_df[column] = temp_df[column].fillna(method='bfill')\n",
    "                logging.info(f\"Applied back fill strategy to column '{column}'\")\n",
    "            \n",
    "            elif strategy == Strategy.FFILL:\n",
    "                temp_df[column] = temp_df[column].fillna(method='ffill')\n",
    "                logging.info(f\"Applied forward fill strategy to column '{column}'\")\n",
    "            \n",
    "            elif strategy == Strategy.DROP_ROW:\n",
    "                temp_df = temp_df.dropna(subset=[column])\n",
    "                logging.info(f\"Dropped rows with missing values in column '{column}'\")\n",
    "            \n",
    "            elif strategy == Strategy.LINEAR_INTERPOLATION:\n",
    "                temp_df[column] = temp_df[column].interpolate(method='linear')\n",
    "                logging.info(f\"Applied linear interpolation to column '{column}'\")\n",
    "            \n",
    "            elif strategy == Strategy.MODE:\n",
    "                mode_value = temp_df[column].mode()[0]\n",
    "                temp_df[column] = temp_df[column].fillna(mode_value)\n",
    "                logging.info(f\"Applied mode imputation to column '{column}' with mode value {mode_value}\")\n",
    "            \n",
    "            elif strategy == Strategy.MEDIAN:\n",
    "                median_value = temp_df[column].median()\n",
    "                temp_df[column] = temp_df[column].fillna(median_value)\n",
    "                logging.info(f\"Applied median imputation to column '{column}' with median value {median_value}\")\n",
    "            \n",
    "            return temp_df\n",
    "        else:\n",
    "            logging.info(f\"Column '{column}' has {missing_ratio * 100:.2f}% missing values, which is lower than the threshold of {threshold * 100:.2f}%. Not applying a strategy.\")\n",
    "            return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while handling missing values in column '{column}' with strategy '{strategy.name}': {e}\")\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:25:25.268512Z",
     "start_time": "2024-11-12T09:25:25.260790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_rows_by_value(df, column_name, column_value):\n",
    "    \"\"\"\n",
    "    Filter rows in a DataFrame based on column value and drop all rows where values do not match the given value.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to filter.\n",
    "    column_name (str): The column name to check for the value.\n",
    "    column_value: The value to filter rows by.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with rows filtered by the given column value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the column exists in the DataFrame\n",
    "        if column_name not in df.columns:\n",
    "            logging.error(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "            return df\n",
    "        \n",
    "        # Create a mask for the matching rows\n",
    "        mask = df[column_name] == column_value\n",
    "        affected_rows = len(df) - mask.sum()  # Calculate the number of rows that do not match\n",
    "        \n",
    "        # Filter the DataFrame\n",
    "        filtered_df = df[mask]\n",
    "        \n",
    "        logging.info(f\"Column '{column_name}': Filtering rows where value is '{column_value}'.\")\n",
    "        logging.info(f\"Number of rows dropped: {affected_rows}\")\n",
    "        return filtered_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while filtering rows by '{column_name}' with value '{column_value}': {e}\")\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:10:03.144294Z",
     "start_time": "2024-11-12T09:10:03.139753Z"
    }
   },
   "source": [
    "# TODO: I will have to see how well this works with the different datasets. Check how many NaT (not a time) values there are, and handle that case (possibly using above function)\n",
    "\n",
    "def filter_transform_to_datetime(df, column):\n",
    "    \"\"\"\n",
    "    Transform the date column in various formats to a uniform datetime datatype.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the date column.\n",
    "    column (str): The column name containing date values in various formats.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with the date column transformed to datetime.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temp_df = copy.deepcopy(df) # Avoid making in place changes to the dataframe\n",
    "        \n",
    "        logging.info(f\"Transforming column '{column}' to datetime\")\n",
    "        temp_df[column] = pd.to_datetime(temp_df[column], errors='coerce')\n",
    "        logging.info(f\"Successfully transformed column '{column}' to datetime\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while transforming column '{column}' to datetime: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return temp_df"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:10:03.279920Z",
     "start_time": "2024-11-12T09:10:03.273532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_df_to_csv(df, file_name, file_path='../data/', overwrite=False):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to save.\n",
    "    file_path (str): The path where the CSV file will be saved. The default path is that to the local /data/ folder, as required by the project specifications.\n",
    "    file_name (str): The name of the file to be stored, excluding the file ending, which is hardcoded as '.csv'\n",
    "    overwrite (bool): Flag to allow overwriting of existing files.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    if not file_path: # Check if file_path is an empty string \n",
    "        file_path = './' # Default to current working directory \n",
    "    \n",
    "    full_path = os.path.join(file_path, file_name + '.csv')\n",
    "    \n",
    "    # Check if the file already exists \n",
    "    if os.path.exists(full_path):\n",
    "        if not overwrite:\n",
    "            logging.error(f\"File '{full_path}' already exists. Set overwrite-flag to True in order to perform this action\") \n",
    "            return\n",
    "        else: \n",
    "            logging.warning(f\"File '{full_path}' is being overwritten as the ovewrite-flag is set to True\")\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        df.to_csv(full_path, index=False)\n",
    "        logging.info(f\"DataFrame successfully saved to {full_path}\")\n",
    "    except PermissionError as e:\n",
    "        logging.error(f\"Permission error while trying to save the DataFrame to {full_path}: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found error while trying to save the DataFrame to {full_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error while saving the DataFrame to {full_path}: {e}\")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Applying the ETL Pipeline to the datasets "
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chile Covid Mortality Dataset"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:16:25.658205Z",
     "start_time": "2024-11-12T09:16:22.110048Z"
    }
   },
   "source": [
    "chile_url = \"https://datos.gob.cl/dataset/8982a05a-91f7-422d-97bc-3eee08fde784/resource/8e5539b7-10b2-409b-ae5a-36dae4faf817/download/defunciones_covid19_2020_2024.csv\"\n",
    "\n",
    "# Extract the dataset into a data-frame\n",
    "chile_data = extract_dataset(chile_url, timeout=(200,200))\n",
    "chile_df = extract_into_df(chile_data, separator=\";\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 10:16:22,112 - INFO - Attempting to fetch data from https://datos.gob.cl/dataset/8982a05a-91f7-422d-97bc-3eee08fde784/resource/8e5539b7-10b2-409b-ae5a-36dae4faf817/download/defunciones_covid19_2020_2024.csv\n",
      "2024-11-12 10:16:22,114 - DEBUG - Starting new HTTPS connection (1): datos.gob.cl:443\n",
      "2024-11-12 10:16:23,016 - DEBUG - https://datos.gob.cl:443 \"GET /dataset/8982a05a-91f7-422d-97bc-3eee08fde784/resource/8e5539b7-10b2-409b-ae5a-36dae4faf817/download/defunciones_covid19_2020_2024.csv HTTP/11\" 200 16071695\n",
      "2024-11-12 10:16:25,468 - INFO - Successfully fetched data from https://datos.gob.cl/dataset/8982a05a-91f7-422d-97bc-3eee08fde784/resource/8e5539b7-10b2-409b-ae5a-36dae4faf817/download/defunciones_covid19_2020_2024.csv\n",
      "2024-11-12 10:16:25,470 - INFO - Attempting to load data into DataFrame\n",
      "2024-11-12 10:16:25,653 - INFO - Successfully loaded data into DataFrame with 58289 rows\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:18:39.543025Z",
     "start_time": "2024-11-12T09:18:39.511570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform transformations\n",
    "\n",
    "# Required fields for analysis are the death-date and the diagnosis (COVID-19)\n",
    "chile_df = filter_drop_columns(chile_df, [\"FECHA_DEF\", \"DIAG1\"])\n",
    "\n",
    "# Transform the date-fields into datetime objects\n",
    "chile_df = filter_transform_to_datetime(chile_df, \"FECHA_DEF\")\n",
    "\n",
    "# No missing values are imputed, as there are not enough missing values in the dataset\n",
    "for column in chile_df.columns:\n",
    "    chile_df = filter_handle_missing_values(chile_df, column=column, strategy=Strategy.DROP_ROW)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 10:18:39,514 - INFO - Attempting to drop non-whitelisted columns\n",
      "2024-11-12 10:18:39,520 - INFO - Successfully dropped columns. Remaining columns: ['FECHA_DEF', 'DIAG1']\n",
      "2024-11-12 10:18:39,522 - INFO - Transforming column 'FECHA_DEF' to datetime\n",
      "2024-11-12 10:18:39,535 - INFO - Successfully transformed column 'FECHA_DEF' to datetime\n",
      "2024-11-12 10:18:39,537 - INFO - Column 'FECHA_DEF' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 10:18:39,540 - INFO - Column 'DIAG1' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FECHA_DEF\n",
      "DIAG1\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:19:08.245977Z",
     "start_time": "2024-11-12T09:19:08.239182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the transformed dataframe back into a CSV-database file.\n",
    "load_df_to_csv(chile_df, file_name='chile_covid_mortality', overwrite=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 10:19:08,242 - ERROR - File '../data/chile_covid_mortality.csv' already exists. Set overwrite-flag to True in order to perform this action\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### USA Covid Mortality Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:44:32.745559Z",
     "start_time": "2024-11-12T09:44:27.904566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "usa_url = \"https://data.cdc.gov/api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD\"\n",
    "\n",
    "# Extract the dataset into a data-frame\n",
    "data = extract_dataset(usa_url, timeout=(200, 200))\n",
    "usa_df = extract_into_df(data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 10:44:27,907 - INFO - Attempting to fetch data from https://data.cdc.gov/api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD\n",
      "2024-11-12 10:44:27,910 - DEBUG - Starting new HTTPS connection (1): data.cdc.gov:443\n",
      "2024-11-12 10:44:29,209 - DEBUG - https://data.cdc.gov:443 \"GET /api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD HTTP/11\" 200 None\n",
      "2024-11-12 10:44:32,628 - INFO - Successfully fetched data from https://data.cdc.gov/api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD\n",
      "2024-11-12 10:44:32,629 - INFO - Attempting to load data into DataFrame\n",
      "2024-11-12 10:44:32,742 - INFO - Successfully loaded data into DataFrame with 79002 rows\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:44:56.087127Z",
     "start_time": "2024-11-12T09:44:56.051444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform transformations\n",
    "\n",
    "# This dataset has duplicate values, therefore drop all rows for the different regions in the US and keep only the total US rows.\n",
    "usa_df = filter_rows_by_value(usa_df, \"jurisdiction_residence\", \"United States\")\n",
    "\n",
    "# Keep only required fields for analysis\n",
    "usa_df = filter_drop_columns(usa_df, [\"data_period_start\", \"data_period_end\", \"group\", \"subgroup1\", \"covid_deaths\", \"crude_rate\"]) \n",
    "\n",
    "# Transform the date-fields into datetime objects. This also works for the american M/D/Y date format.\n",
    "usa_df = filter_transform_to_datetime(usa_df, \"data_period_start\") \n",
    "usa_df = filter_transform_to_datetime(usa_df, \"data_period_end\")\n",
    "\n",
    "# Drop the rows for which there is no data about covid mortality\n",
    "print(f\"Before: \\n{usa_df.isnull().sum()} \\n\")\n",
    "for column in usa_df.columns:\n",
    "    usa_df = filter_handle_missing_values(usa_df, column=column, strategy=Strategy.DROP_ROW)\n",
    "print(f\"After: \\n{usa_df.isnull().sum()} \\n\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 10:44:56,053 - ERROR - Column 'jurisdiction_residence' does not exist in the DataFrame.\n",
      "2024-11-12 10:44:56,055 - INFO - Attempting to drop non-whitelisted columns\n",
      "2024-11-12 10:44:56,057 - INFO - Successfully dropped columns. Remaining columns: ['data_period_start', 'data_period_end', 'group', 'subgroup1', 'covid_deaths', 'crude_rate']\n",
      "2024-11-12 10:44:56,059 - INFO - Transforming column 'data_period_start' to datetime\n",
      "2024-11-12 10:44:56,066 - INFO - Successfully transformed column 'data_period_start' to datetime\n",
      "2024-11-12 10:44:56,067 - INFO - Transforming column 'data_period_end' to datetime\n",
      "2024-11-12 10:44:56,072 - INFO - Successfully transformed column 'data_period_end' to datetime\n",
      "2024-11-12 10:44:56,075 - INFO - Column 'data_period_start' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 10:44:56,076 - INFO - Column 'data_period_end' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 10:44:56,078 - INFO - Column 'group' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 10:44:56,080 - INFO - Column 'subgroup1' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 10:44:56,081 - INFO - Column 'covid_deaths' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n",
      "2024-11-12 10:44:56,082 - INFO - Column 'crude_rate' has 0.00% missing values, which is lower than the threshold of 0.00%. Not applying a strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "data_period_start    0\n",
      "data_period_end      0\n",
      "group                0\n",
      "subgroup1            0\n",
      "covid_deaths         0\n",
      "crude_rate           0\n",
      "dtype: int64 \n",
      "\n",
      "After: \n",
      "data_period_start    0\n",
      "data_period_end      0\n",
      "group                0\n",
      "subgroup1            0\n",
      "covid_deaths         0\n",
      "crude_rate           0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:46:25.909553Z",
     "start_time": "2024-11-12T09:46:25.878975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the transformed dataframe back into a CSV-database file.\n",
    "load_df_to_csv(usa_df, file_name='usa_covid_mortality', overwrite=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 10:46:25,907 - INFO - DataFrame successfully saved to ../data/usa_covid_mortality.csv\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "usa_url = \"https://data.cdc.gov/api/views/exs3-hbne/rows.csv?fourfour=exs3-hbne&cacheBust=1729520760&date=20241106&accessType=DOWNLOAD\"\n",
    "\n",
    "# Extract the dataset into a data-frame\n",
    "data = extract_dataset(usa_url, timeout=(200, 200))\n",
    "usa_df = extract_into_df(data)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Perform transformations\n",
    "\n",
    "# This dataset has duplicate values, therefore drop all rows for the different regions in the US and keep only the total US rows.\n",
    "usa_df = filter_rows_by_value(usa_df, \"jurisdiction_residence\", \"United States\")\n",
    "\n",
    "# Keep only required fields for analysis\n",
    "usa_df = filter_drop_columns(usa_df, [\"data_period_start\", \"data_period_end\", \"group\", \"subgroup1\", \"covid_deaths\", \"crude_rate\"]) \n",
    "\n",
    "# Transform the date-fields into datetime objects. This also works for the american M/D/Y date format.\n",
    "usa_df = filter_transform_to_datetime(usa_df, \"data_period_start\") \n",
    "usa_df = filter_transform_to_datetime(usa_df, \"data_period_end\")\n",
    "\n",
    "# Drop the rows for which there is no data about covid mortality\n",
    "print(f\"Before: \\n{usa_df.isnull().sum()} \\n\")\n",
    "for column in usa_df.columns:\n",
    "    usa_df = filter_handle_missing_values(usa_df, column=column, strategy=Strategy.DROP_ROW)\n",
    "print(f\"After: \\n{usa_df.isnull().sum()} \\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the transformed dataframe back into a CSV-database file.\n",
    "load_df_to_csv(usa_df, file_name='usa_covid_mortality', overwrite=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
